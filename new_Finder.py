import urllib as url
import io
import requests
import webbrowser
import re
import scrapy
import json
import json_Controller as jsc
from scrapy.crawler import CrawlerProcess

def get_Url():
    data = jsc.json_Search_Api('snap+stock+news')
    for links in data['results']:
        print 'title: ', links['title']
        print 'description: ', links['sum']
        print 'link: ', links['url']
        print '-----------------------------'
"""
#Function is currently broken, have to figure out way to pull json
#out of the arry that it gets put in when enumerate happens for it to
#properly process and be useful or we can use regex magic but thats a
#shit method
def get_Url():
    data = jsc.json_Search_Api('snap+stock+news')
    for links in enumerate(data['results']):
        print links
        print '--------------------------------'
        for key, value in [links]:
            #This is fucked up right now we need to figure out a way to add double quotations
            #and replace single quotes without affecting strings which contain them, because
            #that breaks the string and doesn't allow the json to execute properly
            final_value = re.sub (r'\'', '\"', str(value))
            final_final = re.sub(r'u\"', '\"', str(final_value))
            final_final_final = re.sub(r'(.*)\"s', 'ts', final_final)
            print final_final_final
            link_Data = json.loads(final_final_final)
            print value
            #for key, value in link_Data.iteritems():
                #print key
                #print value
"""

class NewsSpider(scrapy.Spider):
    name = 'News Spider'

    def __init__(self, name):
        self.start_urls = [ name ]
        super(NewsSpider, self).__init__()

    def parse(self, response):
        pass
#We want this program to pull data from google and give us a list of urls generated by the google search, at which point they will be put into another spider and processed for useful
#information based of strings inside their text body, after which they will be evaluated by the program and determined if they are potentially positive or negative, at which point
#the program will prompt the user to check them if certainty is under 75% and the use will have the option to either up or down vote the articles. Each of these positive and negative
#points will factor in for the next month of monthly, weekly and daily, price change predictions
def search_Url(name):
    class Spider(scrapy.Spider):
        name = 'Spider'
        allow_domains = ['google.com']
        start_urls = ['https://www.google.com/search?client=ubuntu&channel=fs&ei=7hHIW5nHOoOU8wXq1qTAAQ&q=snap+stock&oq=snap+stock&gs_l=psy-ab.3..35i39k1j0i67k1l2j0l7.454.454.0.788.1.1.0.0.0.0.172.172.0j1.1.0....0...1..64.psy-ab..0.1.171....0.LMlvYhCrofg',]

        def parse(self, response):
            hxs = scrapy.Selector(response)
            all_Links = hxs.xpath('*//a/@href').extract()
            for link in all_Links:
                yield scrapy.http.Request(url=link, callback=print_this_link)

        def print_this_link(self, link):
            print 'Link Extracted! : {0}'.format(link)


"""
def search_Url(name):
    name = urllib.quote_plus(name)
    search = 'https://www.google.com/search?q=' + name
    url_Search = requests.get(search)
    print search
    soup = BeautifulSoup(url_Search.text)
    num = 1
    arr = []

#This formats the urls so they are complete, however after each is generated we need to ensure only the second value is the list create is taken because that is the real url, the other one
#is a google cache and is not helpful
    for link in  soup.find_all("a",href=re.compile("(?<=/url\?q=)(htt.*://.*)")):
        result = re.split(":(?=http)",link["href"].replace("/url?q=",""))
        #final = re.search(r'\'(.*)\', \'(.*?)\'', str(result), '')
        #print final.result(1)
        arr.insert(num ,result)
        num = num + 1
    print 'Array Number 10' + str(arr[10])
"""
