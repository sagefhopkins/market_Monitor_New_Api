import urllib as url
import io
import requests
import webbrowser
import re
import scrapy
import json
import json_Controller as jsc
from scrapy.crawler import CrawlerProcess

# NOTE:
#   the data format is category: [ keyword1, keyword2, ... ]
#   in python its a dictionary key that corresponds to a list of keywords
with open('keywords.json', 'rb') as f:
    KEYWORDS = json.load(f)

# Percentage of words that have to match for it to catch it
# Category specific for now
THRESHOLD = 0.25

def keyword_Search(string, regex):

    matches = re.findall(regex, string)

    if not matches:
        return False

    counter = float(len(matches))
    length = float(len(string.split()))
    percent = counter / length

    print matches
    print counter
    print length
    print percent

    if percent >=  THRESHOLD:
        return True
    else:
        return False

def get_Url(source='api'):
    if source == 'api':
        data = jsc.json_Search_Api('snap+stock+news')
    else:
        with open(source, 'rb') as f:
            data = json.load(f)

    relevant_links = { 'positive': [], 'negative': [] }

    category_regexes = {}

    for category in KEYWORDS.iterkeys():
        category_regexes[category] = re.compile('(' + r'\b|\b'.join(KEYWORDS[category]) + ')')

    for links in data['results']:
        print 'title: ', links['title']
        print 'description: ', links['sum']
        print 'link: ', links['url']
        print '-----------------------------'

        for category in KEYWORDS.iterkeys():
            if keyword_Search(links['title'] + ' ' + links['sum'], category_regexes[category]):
                relevant_links[category].append(links['url'])

    return relevant_links

class NewsSpider(scrapy.Spider):
    name = 'News Spider'

    def __init__(self, name):
        self.start_urls = [ name ]
        super(NewsSpider, self).__init__()

    def parse(self, response):
        pass
#We want this program to pull data from google and give us a list of urls generated by the google search, at which point they will be put into another spider and processed for useful
#information based of strings inside their text body, after which they will be evaluated by the program and determined if they are potentially positive or negative, at which point
#the program will prompt the user to check them if certainty is under 75% and the use will have the option to either up or down vote the articles. Each of these positive and negative
#points will factor in for the next month of monthly, weekly and daily, price change predictions
def search_Url(name):
    class Spider(scrapy.Spider):
        name = 'Spider'
        allow_domains = ['google.com']
        start_urls = ['https://www.google.com/search?client=ubuntu&channel=fs&ei=7hHIW5nHOoOU8wXq1qTAAQ&q=snap+stock&oq=snap+stock&gs_l=psy-ab.3..35i39k1j0i67k1l2j0l7.454.454.0.788.1.1.0.0.0.0.172.172.0j1.1.0....0...1..64.psy-ab..0.1.171....0.LMlvYhCrofg',]

        def parse(self, response):
            hxs = scrapy.Selector(response)
            all_Links = hxs.xpath('*//a/@href').extract()
            for link in all_Links:
                yield scrapy.http.Request(url=link, callback=print_this_link)

        def print_this_link(self, link):
            print 'Link Extracted! : {0}'.format(link)
